{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Yolov1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fEgktmpeVWVF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637630689880,"user_tz":300,"elapsed":19119,"user":{"displayName":"Scott Fang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12574105684316640551"}},"outputId":"9df7e38e-d702-47d0-f4c0-b3823e6957b5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_KopZQsUvtm","executionInfo":{"status":"ok","timestamp":1637630705136,"user_tz":300,"elapsed":5271,"user":{"displayName":"Scott Fang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12574105684316640551"}},"outputId":"087b4a5a-de80-4cba-f63d-44dd96d4bd8c"},"source":["import torch\n","device = 'cuda' if torch.cuda.is_available()==True else 'cpu'\n","device = torch.device(device)\n","print(f'We are using device name \"{device}\"')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["We are using device name \"cuda\"\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wXRpl1u7YbvK","executionInfo":{"status":"ok","timestamp":1637631611507,"user_tz":300,"elapsed":2163,"user":{"displayName":"Scott Fang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12574105684316640551"}},"outputId":"5b9d910a-6e65-439b-f72e-e3da95deb1bc"},"source":["# Model #\n","\n","import torch\n","import torch.nn as nn\n","\n","architecture_config = [\n","    # Tuple: (kernel_size, num_filters, stride, padding)\n","    (7, 64, 2, 3),\n","    \"M\",\n","    (3, 192, 1, 1),\n","    \"M\",\n","    (1, 128, 1, 0),\n","    (3, 256, 1, 1),\n","    (1, 256, 1, 0),\n","    (3, 512, 1, 1),\n","    \"M\",\n","    # List: tuples and then last integer rpresents number of repeats\n","    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n","    (1, 512, 1, 0),\n","    (3, 1024, 1, 1),\n","    \"M\",\n","    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n","    (3, 1024, 1, 1),\n","    (3, 1024, 2, 1),\n","    (3, 1024, 1, 1),\n","    (3, 1024, 1, 1),\n","]\n","\n","\n","class CNNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, **kwargs):\n","        super(CNNBlock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n","        self.batchnorm = nn.BatchNorm2d(out_channels)\n","        self.leakyrelu = nn.LeakyReLU(0.1)\n","\n","    def forward(self, x):\n","        return self.leakyrelu(self.batchnorm(self.conv(x)))   \n","\n","\n","class Yolov1(nn.Module):\n","    def __init__(self, in_channels=3, **kwargs):\n","        super(Yolov1, self).__init__()\n","        self.architecture = architecture_config\n","        self.in_channels = in_channels\n","        self.darknet = self._create_conv_layers(self.architecture)\n","        self.fcs = self._create_fcs(**kwargs)\n","\n","    def forward(self, x):\n","        x = self.darknet(x)\n","        return self.fcs(torch.flatten(x, start_dim=1))\n","\n","    def _create_conv_layers(self, architecture):\n","        layers = []\n","        in_channels = self.in_channels\n","\n","        for x in architecture:\n","            if type(x) == tuple:\n","                layers += [\n","                    CNNBlock(\n","                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3],\n","                    )\n","                ]\n","                in_channels = x[1]\n","\n","            elif type(x) == str:\n","                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n","\n","            elif type(x) == list:\n","                conv1 = x[0]\n","                conv2 = x[1]\n","                num_repeats = x[2]\n","\n","                for _ in range(num_repeats):\n","                    layers += [\n","                        CNNBlock(\n","                            in_channels,\n","                            conv1[1],\n","                            kernel_size=conv1[0],\n","                            stride=conv1[2],\n","                            padding=conv1[3],\n","                        )\n","                    ]\n","                    layers += [\n","                        CNNBlock(\n","                            conv1[1],\n","                            conv2[1],\n","                            kernel_size=conv2[0],\n","                            stride=conv2[2],\n","                            padding=conv2[3],\n","                        )\n","                    ]\n","                    in_channels = conv2[1]\n","\n","        return nn.Sequential(*layers)\n","\n","    def _create_fcs(self, split_size, num_boxes, num_classes):\n","        S, B, C = split_size, num_boxes, num_classes\n","\n","        # In original paper this should be\n","        # nn.Linear(1024*S*S, 4096),\n","        # nn.LeakyReLU(0.1),\n","        # nn.Linear(4096, S*S*(B*5+C))\n","\n","        return nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(1024 * S * S, 496),\n","            nn.Dropout(0.0),\n","            nn.LeakyReLU(0.1),\n","            nn.Linear(496, S * S * (C + B * 5)),\n","        )\n","\n","def test(S=7, B=2, C=20):\n","    model = Yolov1(split_size=S, num_boxes=B, num_classes=C)\n","    x = torch.randn((2, 3, 448, 448))\n","    print(model(x).shape)\n","\n","test()"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 1470])\n"]}]},{"cell_type":"code","metadata":{"id":"69G9E8HFT0yR","executionInfo":{"status":"ok","timestamp":1637630708341,"user_tz":300,"elapsed":700,"user":{"displayName":"Scott Fang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12574105684316640551"}}},"source":["# utils #\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from collections import Counter\n","\n","def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n","    \"\"\"\n","    Calculates intersection over union\n","    Parameters:\n","        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n","        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n","        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n","    Returns:\n","        tensor: Intersection over union for all examples\n","    \"\"\"\n","\n","    if box_format == \"midpoint\":\n","        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n","        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n","        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n","        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n","        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n","        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n","        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n","        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n","\n","    if box_format == \"corners\":\n","        box1_x1 = boxes_preds[..., 0:1]\n","        box1_y1 = boxes_preds[..., 1:2]\n","        box1_x2 = boxes_preds[..., 2:3]\n","        box1_y2 = boxes_preds[..., 3:4]  # (N, 1)\n","        box2_x1 = boxes_labels[..., 0:1]\n","        box2_y1 = boxes_labels[..., 1:2]\n","        box2_x2 = boxes_labels[..., 2:3]\n","        box2_y2 = boxes_labels[..., 3:4]\n","\n","    x1 = torch.max(box1_x1, box2_x1)\n","    y1 = torch.max(box1_y1, box2_y1)\n","    x2 = torch.min(box1_x2, box2_x2)\n","    y2 = torch.min(box1_y2, box2_y2)\n","\n","    # .clamp(0) is for the case when they do not intersect\n","    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n","\n","    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n","    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n","\n","    return intersection / (box1_area + box2_area - intersection + 1e-6)\n","\n","\n","def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n","    \"\"\"\n","    Does Non Max Suppression given bboxes\n","    Parameters:\n","        bboxes (list): list of lists containing all bboxes with each bboxes\n","        specified as [class_pred, prob_score, x1, y1, x2, y2]\n","        iou_threshold (float): threshold where predicted bboxes is correct\n","        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n","        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n","    Returns:\n","        list: bboxes after performing NMS given a specific IoU threshold\n","    \"\"\"\n","\n","    assert type(bboxes) == list\n","\n","    bboxes = [box for box in bboxes if box[1] > threshold]\n","    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n","    bboxes_after_nms = []\n","\n","    while bboxes:\n","        chosen_box = bboxes.pop(0)\n","\n","        bboxes = [\n","            box\n","            for box in bboxes\n","            if box[0] != chosen_box[0]\n","            or intersection_over_union(\n","                torch.tensor(chosen_box[2:]),\n","                torch.tensor(box[2:]),\n","                box_format=box_format,\n","            )\n","            < iou_threshold\n","        ]\n","\n","        bboxes_after_nms.append(chosen_box)\n","\n","    return bboxes_after_nms\n","\n","\n","def mean_average_precision(\n","    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n","):\n","    \"\"\"\n","    Calculates mean average precision \n","    Parameters:\n","        pred_boxes (list): list of lists containing all bboxes with each bboxes\n","        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n","        true_boxes (list): Similar as pred_boxes except all the correct ones \n","        iou_threshold (float): threshold where predicted bboxes is correct\n","        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n","        num_classes (int): number of classes\n","    Returns:\n","        float: mAP value across all classes given a specific IoU threshold \n","    \"\"\"\n","\n","    # list storing all AP for respective classes\n","    average_precisions = []\n","\n","    # used for numerical stability later on\n","    epsilon = 1e-6\n","\n","    for c in range(num_classes):\n","        detections = []\n","        ground_truths = []\n","\n","        # Go through all predictions and targets,\n","        # and only add the ones that belong to the\n","        # current class c\n","        for detection in pred_boxes:\n","            if detection[1] == c:\n","                detections.append(detection)\n","\n","        for true_box in true_boxes:\n","            if true_box[1] == c:\n","                ground_truths.append(true_box)\n","\n","        # find the amount of bboxes for each training example\n","        # Counter here finds how many ground truth bboxes we get\n","        # for each training example, so let's say img 0 has 3,\n","        # img 1 has 5 then we will obtain a dictionary with:\n","        # amount_bboxes = {0:3, 1:5}\n","        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n","\n","        # We then go through each key, val in this dictionary\n","        # and convert to the following (w.r.t same example):\n","        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n","        for key, val in amount_bboxes.items():\n","            amount_bboxes[key] = torch.zeros(val)\n","\n","        # sort by box probabilities which is index 2\n","        detections.sort(key=lambda x: x[2], reverse=True)\n","        TP = torch.zeros((len(detections)))\n","        FP = torch.zeros((len(detections)))\n","        total_true_bboxes = len(ground_truths)\n","        \n","        # If none exists for this class then we can safely skip\n","        if total_true_bboxes == 0:\n","            continue\n","\n","        for detection_idx, detection in enumerate(detections):\n","            # Only take out the ground_truths that have the same\n","            # training idx as detection\n","            ground_truth_img = [\n","                bbox for bbox in ground_truths if bbox[0] == detection[0]\n","            ]\n","\n","            num_gts = len(ground_truth_img)\n","            best_iou = 0\n","\n","            for idx, gt in enumerate(ground_truth_img):\n","                iou = intersection_over_union(\n","                    torch.tensor(detection[3:]),\n","                    torch.tensor(gt[3:]),\n","                    box_format=box_format,\n","                )\n","\n","                if iou > best_iou:\n","                    best_iou = iou\n","                    best_gt_idx = idx\n","\n","            if best_iou > iou_threshold:\n","                # only detect ground truth detection once\n","                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n","                    # true positive and add this bounding box to seen\n","                    TP[detection_idx] = 1\n","                    amount_bboxes[detection[0]][best_gt_idx] = 1\n","                else:\n","                    FP[detection_idx] = 1\n","\n","            # if IOU is lower then the detection is a false positive\n","            else:\n","                FP[detection_idx] = 1\n","\n","        TP_cumsum = torch.cumsum(TP, dim=0)\n","        FP_cumsum = torch.cumsum(FP, dim=0)\n","        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n","        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n","        precisions = torch.cat((torch.tensor([1]), precisions))\n","        recalls = torch.cat((torch.tensor([0]), recalls))\n","        # torch.trapz for numerical integration\n","        average_precisions.append(torch.trapz(precisions, recalls))\n","\n","    return sum(average_precisions) / len(average_precisions)\n","\n","\n","def plot_image(image, boxes):\n","    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n","    im = np.array(image)\n","    height, width, _ = im.shape\n","\n","    # Create figure and axes\n","    fig, ax = plt.subplots(1)\n","    # Display the image\n","    ax.imshow(im)\n","\n","    # box[0] is x midpoint, box[2] is width\n","    # box[1] is y midpoint, box[3] is height\n","\n","    # Create a Rectangle potch\n","    for box in boxes:\n","        box = box[2:]\n","        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n","        upper_left_x = box[0] - box[2] / 2\n","        upper_left_y = box[1] - box[3] / 2\n","        rect = patches.Rectangle(\n","            (upper_left_x * width, upper_left_y * height),\n","            box[2] * width,\n","            box[3] * height,\n","            linewidth=1,\n","            edgecolor=\"r\",\n","            facecolor=\"none\",\n","        )\n","        # Add the patch to the Axes\n","        ax.add_patch(rect)\n","\n","    plt.show()\n","\n","def get_bboxes(\n","    loader,\n","    model,\n","    iou_threshold,\n","    threshold,\n","    pred_format=\"cells\",\n","    box_format=\"midpoint\",\n","    device=\"cuda\",\n","):\n","    all_pred_boxes = []\n","    all_true_boxes = []\n","\n","    # make sure model is in eval before get bboxes\n","    model.eval()\n","    train_idx = 0\n","\n","    for batch_idx, (x, labels) in enumerate(loader):\n","        x = x.to(device)\n","        labels = labels.to(device)\n","\n","        with torch.no_grad():\n","            predictions = model(x)\n","\n","        batch_size = x.shape[0]\n","        true_bboxes = cellboxes_to_boxes(labels)\n","        bboxes = cellboxes_to_boxes(predictions)\n","\n","        for idx in range(batch_size):\n","            nms_boxes = non_max_suppression(\n","                bboxes[idx],\n","                iou_threshold=iou_threshold,\n","                threshold=threshold,\n","                box_format=box_format,\n","            )\n","\n","\n","            #if batch_idx == 0 and idx == 0:\n","            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n","            #    print(nms_boxes)\n","\n","            for nms_box in nms_boxes:\n","                all_pred_boxes.append([train_idx] + nms_box)\n","\n","            for box in true_bboxes[idx]:\n","                # many will get converted to 0 pred\n","                if box[1] > threshold:\n","                    all_true_boxes.append([train_idx] + box)\n","\n","            train_idx += 1\n","\n","    model.train()\n","    return all_pred_boxes, all_true_boxes\n","\n","\n","\n","def convert_cellboxes(predictions, S=7):\n","    \"\"\"\n","    Converts bounding boxes output from Yolo with\n","    an image split size of S into entire image ratios\n","    rather than relative to cell ratios. Tried to do this\n","    vectorized, but this resulted in quite difficult to read\n","    code... Use as a black box? Or implement a more intuitive,\n","    using 2 for loops iterating range(S) and convert them one\n","    by one, resulting in a slower but more readable implementation.\n","    \"\"\"\n","\n","    predictions = predictions.to(\"cpu\")\n","    batch_size = predictions.shape[0]\n","    predictions = predictions.reshape(batch_size, 7, 7, 30)\n","    bboxes1 = predictions[..., 21:25]\n","    bboxes2 = predictions[..., 26:30]\n","    scores = torch.cat(\n","        (predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0\n","    )\n","    best_box = scores.argmax(0).unsqueeze(-1)\n","    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n","    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n","    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n","    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n","    w_y = 1 / S * best_boxes[..., 2:4]\n","    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n","    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n","    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(\n","        -1\n","    )\n","    converted_preds = torch.cat(\n","        (predicted_class, best_confidence, converted_bboxes), dim=-1\n","    )\n","\n","    return converted_preds\n","\n","\n","def cellboxes_to_boxes(out, S=7):\n","    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n","    converted_pred[..., 0] = converted_pred[..., 0].long()\n","    all_bboxes = []\n","\n","    for ex_idx in range(out.shape[0]):\n","        bboxes = []\n","\n","        for bbox_idx in range(S * S):\n","            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n","        all_bboxes.append(bboxes)\n","\n","    return all_bboxes\n","\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_lCDfU5ToRR","executionInfo":{"status":"ok","timestamp":1637630713620,"user_tz":300,"elapsed":141,"user":{"displayName":"Scott Fang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12574105684316640551"}}},"source":["# loss function #\n","# Follow the paper to to the loss function\n","class YoloLoss(nn.Module):\n","    # The original yolo has 7*7 cells, and 2 boundong boxes, with 20 classes\n","    def __init__(self, S=7, B=2, C=20):\n","        super(YoloLoss, self).__init__()\n","        self.S = S\n","        self.B = B\n","        self.C = C\n","        self.mse = nn.MSELoss(reduction=\"sum\")\n","        self.lambda_noobj = 0.5\n","        self.lambda_coord = 5\n","\n","    def forward(self, predictions, target):\n","        # reshape the prediction to 7*7*30\n","        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B*5)\n","\n","        # the intersection over union for bouding box\n","        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n","        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n","        #Concatnate iou b1 and iou b2\n","        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n","        iou_maxes, bestbox = torch.max(ious, dim=0)\n","        exists_box = target[..., 20].unsqueeze(3)\n","\n","        #   FOR BOX COORDINATES    #\n","\n","        # Set boxes with no object in them to 0. We only take out one of the two \n","        # predictions, which is the one with highest Iou calculated previously.\n","        box_predictions = exists_box * (\n","            (\n","                bestbox * predictions[..., 26:30]\n","                + (1 - bestbox) * predictions[..., 21:25]\n","            )\n","        )\n","\n","        box_targets = exists_box * target[..., 21:25]\n","\n","        # Take sqrt of width, height of boxes to ensure that\n","        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n","            torch.abs(box_predictions[..., 2:4] + 1e-6)\n","        )\n","        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n","\n","        box_loss = self.mse(\n","            torch.flatten(box_predictions, end_dim=-2),\n","            torch.flatten(box_targets, end_dim=-2),\n","        )\n","\n","        #   FOR OBJECT LOSS    #\n","\n","        # pred_box is the confidence score for the bbox with highest IoU\n","        pred_box = (\n","            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n","        )\n","\n","        object_loss = self.mse(\n","            torch.flatten(exists_box * pred_box),\n","            torch.flatten(exists_box * target[..., 20:21]),\n","        )\n","\n","        #   FOR NO OBJECT LOSS    #\n","\n","        no_object_loss = self.mse(\n","            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n","            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n","        )\n","\n","        no_object_loss += self.mse(\n","            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n","            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n","        )\n","\n","        #   FOR CLASS LOSS   #\n","\n","        class_loss = self.mse(\n","            torch.flatten(exists_box * predictions[..., :20], end_dim=-2,),\n","            torch.flatten(exists_box * target[..., :20], end_dim=-2,),\n","        )\n","\n","        loss = (\n","            self.lambda_coord * box_loss  # first two rows in paper\n","            + object_loss  # third row in paper\n","            + self.lambda_noobj * no_object_loss  # forth row\n","            + class_loss  # fifth row\n","        )\n","\n","        return loss\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"eH29r4HIUNv8","executionInfo":{"status":"ok","timestamp":1637630716626,"user_tz":300,"elapsed":149,"user":{"displayName":"Scott Fang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12574105684316640551"}}},"source":["# dataset #\n","import os\n","import pandas as pd\n","from PIL import Image\n","\n","\n","class VOCDataset(torch.utils.data.Dataset):\n","    def __init__(\n","        self, csv_file, img_dir, label_dir, S=7, B=2, C=20, transform=None,\n","    ):\n","        self.annotations = pd.read_csv(csv_file)\n","        self.img_dir = img_dir\n","        self.label_dir = label_dir\n","        self.transform = transform\n","        self.S = S\n","        self.B = B\n","        self.C = C\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, index):\n","        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n","        boxes = []\n","        with open(label_path) as f:\n","            for label in f.readlines():\n","                class_label, x, y, width, height = [\n","                    float(x) if float(x) != int(float(x)) else int(x)\n","                    for x in label.replace(\"\\n\", \"\").split()\n","                ]\n","\n","                boxes.append([class_label, x, y, width, height])\n","\n","        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n","        image = Image.open(img_path)\n","        boxes = torch.tensor(boxes)\n","\n","        if self.transform:\n","            # image = self.transform(image)\n","            image, boxes = self.transform(image, boxes)\n","\n","        # Convert To Cells\n","        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n","        for box in boxes:\n","            class_label, x, y, width, height = box.tolist()\n","            class_label = int(class_label)\n","\n","            # i,j represents the cell row and cell column\n","            i, j = int(self.S * y), int(self.S * x)\n","            x_cell, y_cell = self.S * x - j, self.S * y - i\n","\n","            \"\"\"\n","            Calculating the width and height of cell of bounding box,\n","            relative to the cell is done by the following, with\n","            width as the example:\n","            \n","            width_pixels = (width*self.image_width)\n","            cell_pixels = (self.image_width)\n","            \n","            Then to find the width relative to the cell is simply:\n","            width_pixels/cell_pixels, simplification leads to the\n","            formulas below.\n","            \"\"\"\n","            width_cell, height_cell = (\n","                width * self.S,\n","                height * self.S,\n","            )\n","\n","            # If no object already found for specific cell i,j\n","            # Note: This means we restrict to ONE object\n","            # per cell!\n","            if label_matrix[i, j, 20] == 0:\n","                # Set that there exists an object\n","                label_matrix[i, j, 20] = 1\n","\n","                # Box coordinates\n","                box_coordinates = torch.tensor(\n","                    [x_cell, y_cell, width_cell, height_cell]\n","                )\n","\n","                label_matrix[i, j, 21:25] = box_coordinates\n","\n","                # Set one hot encoding for class_label\n","                label_matrix[i, j, class_label] = 1\n","\n","        return image, label_matrix"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e02FLeJ6g19e","executionInfo":{"status":"ok","timestamp":1637632117406,"user_tz":300,"elapsed":486641,"user":{"displayName":"Scott Fang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12574105684316640551"}},"outputId":"46c4c1dd-f7ad-4d1a-b3e8-56ab30d09bfb"},"source":["\"\"\"\n","Main file for training Yolo model on Pascal VOC dataset\n","\"\"\"\n","# Train Cell\n","\n","import torch\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import torchvision.transforms.functional as FT\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","\n","seed = 123\n","torch.manual_seed(seed)\n","\n","# Hyperparameters etc. \n","LEARNING_RATE = 2e-5\n","DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n","BATCH_SIZE = 6 # 64 in original paper but I don't have that much vram, grad accum?\n","WEIGHT_DECAY = 0\n","EPOCHS = 30\n","NUM_WORKERS = 2\n","PIN_MEMORY = True\n","LOAD_MODEL = False\n","LOAD_MODEL_FILE = \"/content/drive/MyDrive/ECE570/YOLOv1/overfit.pth.tar\"\n","IMG_DIR = \"/content/drive/MyDrive/ECE570/YOLOv1/data/images\"\n","LABEL_DIR = \"/content/drive/MyDrive/ECE570/YOLOv1/data/labels\"\n","\n","\n","class Compose(object):\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, img, bboxes):\n","        for t in self.transforms:\n","            img, bboxes = t(img), bboxes\n","\n","        return img, bboxes\n","\n","\n","transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])\n","\n","\n","def train_fn(train_loader, model, optimizer, loss_fn):\n","    loop = tqdm(train_loader, leave=True)\n","    mean_loss = []\n","\n","    for batch_idx, (x, y) in enumerate(loop):\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","        out = model(x)\n","        loss = loss_fn(out, y)\n","        mean_loss.append(loss.item())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # update progress bar\n","        loop.set_postfix(loss=loss.item())\n","\n","    print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")\n","\n","\n","def main():\n","    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n","    optimizer = optim.Adam(\n","        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n","    )\n","    loss_fn = YoloLoss()\n","\n","    if LOAD_MODEL:\n","        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n","\n","    train_dataset = VOCDataset(\n","        \"/content/drive/MyDrive/ECE570/YOLOv1/data/100examples.csv\",\n","        transform=transform,\n","        img_dir=IMG_DIR,\n","        label_dir=LABEL_DIR,\n","    )\n","\n","    test_dataset = VOCDataset(\n","        \"/content/drive/MyDrive/ECE570/YOLOv1/data/test.csv\", transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR,\n","    )\n","\n","    train_loader = DataLoader(\n","        dataset=train_dataset,\n","        batch_size=BATCH_SIZE,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        shuffle=True,\n","        drop_last=False,\n","    )\n","\n","    test_loader = DataLoader(\n","        dataset=test_dataset,\n","        batch_size=BATCH_SIZE,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        shuffle=True,\n","        drop_last=False,\n","    )\n","\n","    for epoch in range(EPOCHS):\n","\n","        pred_boxes, target_boxes = get_bboxes(\n","            train_loader, model, iou_threshold=0.5, threshold=0.4\n","        )\n","\n","        mean_avg_prec = mean_average_precision(\n","            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n","        )\n","        print(f\"Train mAP: {mean_avg_prec}\")\n","\n","        if mean_avg_prec > 0.1:\n","            checkpoint = {\n","                \"state_dict\": model.state_dict(),\n","                \"optimizer\": optimizer.state_dict(),\n","            }\n","            save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n","            import time\n","            time.sleep(10)\n","\n","        train_fn(train_loader, model, optimizer, loss_fn)\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Train mAP: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  7.21it/s, loss=100]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 291.9203703138563\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  7.19it/s, loss=55.3]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 167.92621273464627\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  7.29it/s, loss=19.9]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 131.17287826538086\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 5.465023423312232e-05\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  7.26it/s, loss=17.6]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 103.13480589124892\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.01276390254497528\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  7.22it/s, loss=9.27]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 81.47246901194255\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.06414695084095001\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  7.22it/s, loss=13.9]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 70.05614609188504\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.15476761758327484\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.84it/s, loss=29.1]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 62.045266469319664\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.1865791380405426\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.81it/s, loss=11.8]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 57.14341688156128\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.20396240055561066\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.82it/s, loss=7.86]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 50.301098081800674\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.16603462398052216\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.79it/s, loss=4.77]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 48.598655727174545\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.3053800165653229\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.91it/s, loss=16.8]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 43.52904648251004\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.42122799158096313\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.81it/s, loss=11.6]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 42.002747217814125\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.40873685479164124\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.70it/s, loss=10.4]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 41.70587052239312\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.4673817753791809\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.74it/s, loss=47.4]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 40.690056482950844\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.5343157649040222\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.55it/s, loss=17.3]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 36.46227094862196\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.5712514519691467\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.57it/s, loss=7.8]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 34.6546397474077\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.576285719871521\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.56it/s, loss=3.97]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 32.79954342047373\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.6723117828369141\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.67it/s, loss=5.95]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 31.56253769662645\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.6172365546226501\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.46it/s, loss=15.3]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 30.49464374118381\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.6304120421409607\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.57it/s, loss=14.2]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 28.886228826310898\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.7465680837631226\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.67it/s, loss=9.41]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 30.446213828192818\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.6642615795135498\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.55it/s, loss=6.51]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 30.834837357203167\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.6949346661567688\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.88it/s, loss=2.23]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 25.770799252722\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.6909968852996826\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.57it/s, loss=6.18]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 25.764835410647922\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.7639394998550415\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.78it/s, loss=7.82]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 26.699091407987808\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.603212296962738\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.56it/s, loss=4.48]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 28.575905111100937\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.6584262847900391\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.83it/s, loss=3.31]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 32.60007209248013\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.6955617666244507\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.76it/s, loss=5.64]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 31.15403151512146\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.7022315859794617\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.77it/s, loss=12.7]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 26.472051037682427\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Train mAP: 0.7444620728492737\n","=> Saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:02<00:00,  6.66it/s, loss=4.01]"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 24.686140537261963\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":694},"id":"g7-fBgsH-vk1","executionInfo":{"status":"error","timestamp":1637631577149,"user_tz":300,"elapsed":2272,"user":{"displayName":"Scott Fang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12574105684316640551"}},"outputId":"0ae5ef50-998a-4391-dc9b-9ca06992282b"},"source":["# Test cell\n","class Compose(object):\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, img, bboxes):\n","        for t in self.transforms:\n","            img, bboxes = t(img), bboxes\n","\n","        return img, bboxes\n","\n","\n","transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])\n","\n","\n","def test_fn(test_loader, model, optimizer, loss_fn):\n","    mean_loss = []\n","\n","    for batch_idx, (x, y) in enumerate(test_loader):\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","        out = model(x)\n","        loss = loss_fn(out, y)\n","        mean_loss.append(loss.item())\n","        optimizer.zero_grad()\n","\n","    print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")\n","\n","def main():\n","    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n","    optimizer = optim.Adam(\n","        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n","    )\n","    loss_fn = YoloLoss()\n","    load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n","\n","    train_dataset = VOCDataset(\n","        \"/content/drive/MyDrive/ECE570/YOLOv1/data/100examples.csv\",\n","        transform=transform,\n","        img_dir=IMG_DIR,\n","        label_dir=LABEL_DIR,\n","    )\n","\n","    test_dataset = VOCDataset(\n","        \"/content/drive/MyDrive/ECE570/YOLOv1/data/8examples.csv\", transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR,\n","    )\n","\n","    train_loader = DataLoader(\n","        dataset=train_dataset,\n","        batch_size=BATCH_SIZE,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        shuffle=True,\n","        drop_last=False,\n","    )\n","\n","    test_loader = DataLoader(\n","        dataset=test_dataset,\n","        batch_size=BATCH_SIZE,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        shuffle=True,\n","        drop_last=False,\n","    )\n","\n","    for epoch in range(EPOCHS):\n","        test_fn(test_loader, model, optimizer, loss_fn)\n","        pred_boxes, target_boxes = get_bboxes(\n","            test_loader, model, iou_threshold=0.5, threshold=0.4\n","        )\n","\n","        mean_avg_prec = mean_average_precision(\n","            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n","        )\n","        print(f\"Test mAP: {mean_avg_prec}\")\n","        for x, y in test_loader:\n","            x = x.to(DEVICE)\n","            for idx in range(6):\n","                bboxes = cellboxes_to_boxes(model(x))\n","                bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n","                plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n","            print(f\"Test mAP: {mean_avg_prec}\")\n","\n","            import sys\n","            sys.exit()\n","        \n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["=> Loading checkpoint\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-a3a38e0e5f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-a3a38e0e5f1b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYoloLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOAD_MODEL_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     train_dataset = VOCDataset(\n","\u001b[0;32m<ipython-input-4-870811585e3d>\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(checkpoint, model, optimizer)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=> Loading checkpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1483\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Yolov1:\n\tUnexpected key(s) in state_dict: \"darknet.0.batchnorm.weight\", \"darknet.0.batchnorm.bias\", \"darknet.0.batchnorm.running_mean\", \"darknet.0.batchnorm.running_var\", \"darknet.0.batchnorm.num_batches_tracked\", \"darknet.2.batchnorm.weight\", \"darknet.2.batchnorm.bias\", \"darknet.2.batchnorm.running_mean\", \"darknet.2.batchnorm.running_var\", \"darknet.2.batchnorm.num_batches_tracked\", \"darknet.4.batchnorm.weight\", \"darknet.4.batchnorm.bias\", \"darknet.4.batchnorm.running_mean\", \"darknet.4.batchnorm.running_var\", \"darknet.4.batchnorm.num_batches_tracked\", \"darknet.5.batchnorm.weight\", \"darknet.5.batchnorm.bias\", \"darknet.5.batchnorm.running_mean\", \"darknet.5.batchnorm.running_var\", \"darknet.5.batchnorm.num_batches_tracked\", \"darknet.6.batchnorm.weight\", \"darknet.6.batchnorm.bias\", \"darknet.6.batchnorm.running_mean\", \"darknet.6.batchnorm.running_var\", \"darknet.6.batchnorm.num_batches_tracked\", \"darknet.7.batchnorm.weight\", \"darknet.7.batchnorm.bias\", \"darknet.7.batchnorm.running_mean\", \"darknet.7.batchnorm.running_var\", \"darknet.7.batchnorm.num_batches_tracked\", \"darknet.9.batchnorm.weight\", \"darknet.9.batchnorm.bias\", \"darknet.9.batchnorm.running_mean\", \"darknet.9.batchnorm.running_var\", \"darknet.9.batchnorm.num_batches_tracked\", \"darknet.10.batchnorm.weight\", \"darknet.10.batchnorm.bias\", \"darknet.10.batchnorm.running_mean\", \"darknet.10.batchnorm.running_var\", \"darknet.10.batchnorm.num_batches_tracked\", \"darknet.11.batchnorm.weight\", \"darknet.11.batchnorm.bias\", \"darknet.11.batchnorm...."]}]}]}